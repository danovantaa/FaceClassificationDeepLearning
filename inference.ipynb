{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageOps\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
        "import mediapipe as mp\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "TARGET_SIZE = (384, 384)\n",
        "MARGIN_RATIO = 0.15\n",
        "MAX_DIM = 1600\n",
        "MIN_DIM = 256\n",
        "USE_FACE_ALIGNMENT = True\n",
        "TEST_DIR = \"./Test\"\n"
      ],
      "metadata": {
        "id": "Cp5KYq6IbSN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scan_test_dataset(root_dir):\n",
        "    image_paths = []\n",
        "    true_labels = []\n",
        "\n",
        "    classes = sorted(os.listdir(root_dir))\n",
        "    classes = [c for c in classes if os.path.isdir(os.path.join(root_dir, c))]\n",
        "\n",
        "    print(\"Found classes:\", classes)\n",
        "\n",
        "    for cls in classes:\n",
        "        cls_dir = os.path.join(root_dir, cls)\n",
        "        for fname in os.listdir(cls_dir):\n",
        "            if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".webp\")):\n",
        "                image_paths.append(os.path.join(cls_dir, fname))\n",
        "                true_labels.append(cls)\n",
        "\n",
        "    return image_paths, true_labels"
      ],
      "metadata": {
        "id": "pkcnUXs-bgB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mp_face_detection = mp.solutions.face_detection\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "face_detection = mp_face_detection.FaceDetection(\n",
        "    model_selection=1,\n",
        "    min_detection_confidence=0.5\n",
        ")\n",
        "\n",
        "face_mesh = mp_face_mesh.FaceMesh(\n",
        "    static_image_mode=True,\n",
        "    max_num_faces=1,\n",
        "    refine_landmarks=True,\n",
        "    min_detection_confidence=0.5\n",
        ")\n",
        "\n",
        "def CenterEye(landmarks, image_shape):\n",
        "    h, w = image_shape[:2]\n",
        "    EYE_LEFT_IDX = [33, 133, 160, 159, 158, 157, 173, 144, 145, 153]\n",
        "    EYE_RIGHT_IDX = [362, 263, 387, 386, 385, 384, 398, 373, 374, 380]\n",
        "\n",
        "    left = np.mean([[landmarks.landmark[i].x * w, landmarks.landmark[i].y * h] for i in EYE_LEFT_IDX], axis=0)\n",
        "    right = np.mean([[landmarks.landmark[i].x * w, landmarks.landmark[i].y * h] for i in EYE_RIGHT_IDX], axis=0)\n",
        "    return left, right\n",
        "\n",
        "def FaceAlign(image, left_eye, right_eye):\n",
        "    dY = right_eye[1] - left_eye[1]\n",
        "    dX = right_eye[0] - left_eye[0]\n",
        "    angle = np.degrees(np.arctan2(dY, dX))\n",
        "\n",
        "    eyes_center = ((left_eye[0] + right_eye[0]) // 2,\n",
        "                   (left_eye[1] + right_eye[1]) // 2)\n",
        "\n",
        "    h, w = image.shape[:2]\n",
        "    M = cv2.getRotationMatrix2D(eyes_center, angle, 1.0)\n",
        "    return cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC), angle\n",
        "\n",
        "def DetectAlignCrop(image_path):\n",
        "    print(f\"→ {os.path.basename(image_path)}\", end=\" | \")\n",
        "\n",
        "    try:\n",
        "        pil_img = Image.open(image_path)\n",
        "        pil_img = ImageOps.exif_transpose(pil_img)\n",
        "        pil_img = pil_img.convert(\"RGB\")\n",
        "        img = np.array(pil_img)\n",
        "    except:\n",
        "        print(\"Load ERROR\")\n",
        "        return None\n",
        "\n",
        "    orig_img = img.copy()\n",
        "\n",
        "    results_mesh = face_mesh.process(img)\n",
        "    if results_mesh.multi_face_landmarks:\n",
        "        left, right = CenterEye(results_mesh.multi_face_landmarks[0], img)\n",
        "        img, _ = FaceAlign(img, left, right)\n",
        "        print(\"Align\", end=\" | \")\n",
        "\n",
        "    # Face detection\n",
        "    results = face_detection.process(img)\n",
        "    if not results.detections:\n",
        "        print(\"No-face\")\n",
        "        return orig_img\n",
        "\n",
        "    det = results.detections[0]\n",
        "    bbox = det.location_data.relative_bounding_box\n",
        "\n",
        "    h, w = img.shape[:2]\n",
        "    x = int(bbox.xmin * w)\n",
        "    y = int(bbox.ymin * h)\n",
        "    bw = int(bbox.width * w)\n",
        "    bh = int(bbox.height * h)\n",
        "\n",
        "    mx = int(bw * MARGIN_RATIO)\n",
        "    my = int(bh * MARGIN_RATIO)\n",
        "\n",
        "    x1 = max(0, x - mx)\n",
        "    y1 = max(0, y - my)\n",
        "    x2 = min(w, x + bw + mx)\n",
        "    y2 = min(h, y + bh + my)\n",
        "\n",
        "    cropped = img[y1:y2, x1:x2]\n",
        "    print(\"OK\")\n",
        "    return cropped"
      ],
      "metadata": {
        "id": "cYZ5-Q2gbeuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"DeepLearningTubes/Results/class_labels.json\") as f:\n",
        "    label_map = json.load(f)\n",
        "\n",
        "idx_to_label = {v: k for k, v in label_map.items()}\n",
        "num_classes = len(label_map)"
      ],
      "metadata": {
        "id": "wxOeEwUqbdGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225]\n",
        "    ),\n",
        "])"
      ],
      "metadata": {
        "id": "Fc7eXMvvbcHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "class ArcMarginProduct(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementasi ArcFace head:\n",
        "    cos(theta + m) * s\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, s=25.0, m=0.10, easy_margin=False):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "        self.easy_margin = easy_margin\n",
        "        self.cos_m = torch.cos(torch.tensor(m))\n",
        "        self.sin_m = torch.sin(torch.tensor(m))\n",
        "        self.th = torch.cos(torch.tensor(3.14159265 - m))\n",
        "        self.mm = torch.sin(torch.tensor(3.14159265 - m)) * m\n",
        "\n",
        "    def forward(self, embeddings, labels):\n",
        "        embeddings = F.normalize(embeddings)\n",
        "        W = F.normalize(self.weight)\n",
        "\n",
        "        cosine = F.linear(embeddings, W)\n",
        "        sine = torch.sqrt(1.0 - cosine**2)\n",
        "\n",
        "        phi = cosine * self.cos_m - sine * self.sin_m\n",
        "\n",
        "        if self.easy_margin:\n",
        "            phi = torch.where(cosine > 0, phi, cosine)\n",
        "        else:\n",
        "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
        "\n",
        "        one_hot = torch.zeros_like(cosine)\n",
        "        one_hot.scatter_(1, labels.view(-1,1), 1)\n",
        "\n",
        "        logits = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
        "        logits = logits * self.s\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "Duf9lb96b7p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet100ArcFace(nn.Module):\n",
        "    \"\"\"\n",
        "    Backbone: ResNet101 → embedding 512\n",
        "    Head: ArcFace (ArcMarginProduct)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes: int,\n",
        "                 embedding_dim: int = 512,\n",
        "                 s: float = 25.0,\n",
        "                 m: float = 0.10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Gunakan ResNet101 pretrained\n",
        "        weights = models.ResNet101_Weights.IMAGENET1K_V1\n",
        "        backbone = models.resnet101(weights=weights)\n",
        "        embedding_in_features = backbone.fc.in_features\n",
        "\n",
        "        # Replace fully connected layer\n",
        "        backbone.fc = nn.Identity()\n",
        "        self.backbone = backbone\n",
        "\n",
        "        # Embedding → 512 dim\n",
        "        self.embedding_head = nn.Sequential(\n",
        "            nn.Linear(embedding_in_features, embedding_dim),\n",
        "            nn.BatchNorm1d(embedding_dim)\n",
        "        )\n",
        "\n",
        "        nn.init.constant_(self.embedding_head[1].weight, 1.0)\n",
        "        nn.init.constant_(self.embedding_head[1].bias, 0.0)\n",
        "        self.embedding_head[1].bias.requires_grad = False\n",
        "\n",
        "        # ArcFace head\n",
        "        self.arc_margin = ArcMarginProduct(\n",
        "            in_features=embedding_dim,\n",
        "            out_features=num_classes,\n",
        "            s=s,\n",
        "            m=m\n",
        "        )\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        x = self.backbone(x)\n",
        "        emb = self.embedding_head(x)\n",
        "        emb = F.normalize(emb, dim=1)\n",
        "\n",
        "        if labels is None:\n",
        "            # inference → tanp margin (cosine similarity * s)\n",
        "            logits = F.linear(\n",
        "                emb,\n",
        "                F.normalize(self.arc_margin.weight)\n",
        "            ) * self.arc_margin.s\n",
        "        else:\n",
        "            logits = self.arc_margin(emb, labels)\n",
        "\n",
        "        return logits, emb\n"
      ],
      "metadata": {
        "id": "2wLGZ7g1cRK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet100ArcFace(num_classes, 512, s=25.0, m=0.0).to(device)\n",
        "model.load_state_dict(torch.load(\"DeepLearningTubes/Models/Resnet100ArcFace.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded!\")"
      ],
      "metadata": {
        "id": "r_frHzcebZDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_paths, test_true_labels = scan_test_dataset(TEST_DIR)\n",
        "\n",
        "all_preds = []\n",
        "all_trues = []\n",
        "results = []\n",
        "\n",
        "for path, true_name in zip(test_paths, test_true_labels):\n",
        "\n",
        "    cropped = DetectAlignCrop(path)\n",
        "    if cropped is None:\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "    else:\n",
        "        img = Image.fromarray(cropped)\n",
        "\n",
        "    img = val_tf(img).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits, _ = model(img, labels=None)\n",
        "        pred_idx = logits.argmax(1).item()\n",
        "\n",
        "    all_preds.append(pred_idx)\n",
        "    all_trues.append(label_map[true_name])\n",
        "\n",
        "    # SAVE CSV DATA\n",
        "    results.append({\n",
        "        \"filename\": os.path.basename(path),\n",
        "        \"label\": idx_to_label[pred_idx]\n",
        "    })\n"
      ],
      "metadata": {
        "id": "XIyDKOekbV4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(results).to_csv(\"output.csv\", index=False)\n",
        "print(\"output.csv saved!\")\n",
        "\n",
        "# METRICS\n",
        "acc = accuracy_score(all_trues, all_preds)\n",
        "prec = precision_score(all_trues, all_preds, average=\"macro\")\n",
        "rec = recall_score(all_trues, all_preds, average=\"macro\")\n",
        "f1 = f1_score(all_trues, all_preds, average=\"macro\")\n",
        "\n",
        "print(\"\\nRESULTS:\")\n",
        "print(\"Accuracy :\", acc)\n",
        "print(\"Precision:\", prec)\n",
        "print(\"Recall   :\", rec)\n",
        "print(\"F1 Score :\", f1)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(all_trues, all_preds))"
      ],
      "metadata": {
        "id": "WxaXqBrLbUgT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}